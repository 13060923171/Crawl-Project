# 爬取百度文库

## Before use:

```python
pip install -r requirements.txt
```

### 内容说明

>[1] [src](./src)文件夹为源代码&参考资料
>
>[2]User-Agent和Robot协议请看[Robot](./Robot.md)
>
>[3]Ajax请看[Ajax补充小知识](./Ajax知识点补充.md)
>
>[4]JSP请看[JSP知识点补充](./JSP知识的小补充.md)
>
> [5] [爬取案例](./爬取案例)为已爬取的案例，原文url在源文件中
>
>源文件位置：**./src/点这里→爬取百度文库.py**



### 使用说明

在源文件main函数内输入爬取的url地址：

![image-20200428102405441](./README/image-20200428102405441.png)

当需要爬取文本文档时，打开下方注释：

![image-20200428102549413](./README/image-20200428102549413.png)

注：代码仍有一些部分无法爬取，目前已知的有一些pdf文档无法爬取，原因是网页结构有所不同，暂时未全部考虑。除此之外，此类付费文档无法爬取：

![付费文档](./README/付费文档.png)

如果还有无法爬取的内容，欢迎及时反馈作者。作者邮箱在文档底部。

如想深入了解代码原理，请阅读下文。

## 学习经历

> 在整个过程中，我们经历了很多东西，但是好在现在已经完成了:joy:。
>
> 下面我们分板块进行一些总结以及一些坑，便于大家学习。
>
> 本文纯属技术分享，无任何商业用途，不承担任何法律责任，也希望同学们不要用作商业用途。



*长文预警！！！！*:artificial_satellite:



注：本段滑稽语气、丑陋排版皆为xkw成果，与zll无关。在此声明。

### 文本文件（TXT & DOCX）

> ​	在爬取`TXT`和`DOCX`文件的时候，我们采取的思路和后面爬取`PDF`和`PPT`的
>
> 想法类似就利用自动化测试工具`seleium`去爬取，但是，我们发现了知乎
>
> 上面有一篇文章[1]介绍了如何爬取`TXT`和`DOCX`，并没有使用`seleium`，我
>
> 们就很好奇，研究了代码以后发现，原因是User-Agent[2]的不同。
>
> ​	百度文库是一个很神奇的地方，如果我们使用我们自己的User-Agent我们
>
> 只能访问到我们记忆中的那个页面，就是没有内容，一堆Ajax[3]异步文
>
> 件。
>
> ​	但是用`BaiduSpider`这个User-Agent就不一样了，顾名思义，这个就是百
>
> 度搜索的User-Agent，那么为什么我们使用这个就可以爬到呢？用自己的就
>
> 爬不到呢？是不是用百度这个User-Agent我们就可以获取其他所有类型的
>
> 呢？在进一步测试中发现，不是的，这个只对文本类有用，原因是什么
>
> 呢？
>
> ​	在研究了`robot`[2]协议之后，再加上观察直接用百度搜索搜寻一个`TXT`
>
> 类文件的时候，确实可以搜到文件中的内容，我们可以大胆猜想，
>
> `BaiduiSpider`这个User-Agent其实就是将我们伪装成百度的搜索引擎，
>
> 那么我们利用这个User-Agent对网址发起请求和百度搜索引擎发起请求是一
>
> 个效果，都可以获得文本内容。
>
> ​	后面获得文本内容了之后就很简单了，找文本，存为DOCX文件等等操作
>
> 都很简单，也没有什么雷踩的，就不多赘述了。

### 图片文件（PDF & PPT）

> ​	说起图片类文件，真的是一把心酸泪，诶。
>
> ​	前文已经说过了，直接利用`User-Agent`这个方法行不通，那怎么办？对
>
> 于这种情况我们一般来说两种办法。
>
> ​	第一种就是找到异步发送请求的`URL`，观察规律，直接向它发送请求，我
>
> 们可以获得图片，但是这种太难实现了，就对于我们一般来说，要找到期
>
> 中的规律，几乎不可能，当然，附件里面的参考文章里面是使用的这种方
>
> 法，我们参考了以后觉得这种方法不现实，因为就普通人而言，很难做到
>
> 这一点的，如果你是一个前端的大神，那就请跳过我刚刚的话。
>
> ​	第二种就是我们这种菜鸟用的`seleium`了。通过`seleium`，我们模仿进入
>
> 目标网页，然后动态的获取源代码，捕获目标。
>
> ​	第二种方法是不是听起来很简单？对于使用过`seleium`这个库的同学来
>
> 说，这肯定是一个很常规的操作。但是，请你务必要先尝试一下自己的想
>
> 法，再回来继续看，因为只有那样，下面对你而言才是最有作用的。
>
> ​	为什么说这个操作听起来简单，实现起来难呢？相信如果你按照上面所
>
> 说的已经实现了一次自己的程序以后，已经有所感受了。
>
> ​	下面列一下我们整个过程。
>
> ​	模拟进入`URL`   ---->  点击继续阅读    ---->  获取全部页数   ---->  循环换
>
> 页+爬取过程     ---->  保存为图片   ---->  转为`PDF`
>
> ​	别急，虽然只有四个过程，但是其中的坑，是你无法想象的多。
>
> ​	按照过程的顺序，我们把遇到的坑以及解决的办法一一列举出来。
>
> ​	模拟进入`URL`这个步骤可以说有坑，也可以说无坑，这里需要提醒的是，
>
> 如果出现`webdriver`失败的问题，多半是没有加入系统变量或者如果懒得
>
> 加入系统变量的话，直接在（）中填写文件所在的位置也可以，注意一定
>
> 要精确到`.exe`。如果你之前已经解决了这个问题，那么这个问题就不是
>
> 坑，如果没有，可能又要耽误一点时间了。
>
> ​	点击继续阅读，这个位置其实是技巧性很强的，如果我们直接通过`xpath`
>
> 锁定这个按钮，然后按照正常的那种`.click`的话，行不通，不行的话，自
>
> 己去试试，原因是什么？我也没有仔细研究过，我猜测是反爬机制吧。那
>
> 么我们怎么解决的？我们是调用`JSP`[4]脚本来实现点击的，具体的实现同学们
>
> 可以在源代码中找到，如果在这个位置你不知道应该通过脚本`JSP`脚本实
>
> 现的话，ok，恭喜你，你的爬取已经提前结束了。
>
> ​	获取全部页数，怎么说呢，这个位置可以说是坑，也可以说不是坑，在
>
> 这里需要先介绍一下，我们为什么在已经点击继续阅读的前提下，还要去
>
> 获取全部页面实现换页的操作。
>
> ​	原因还是一样，我希望你自己先去研究一下源代码的规律，自己尝试着
>
> 在检查页面去按`Ctrl F`搜索一下图片，观察一下源代码的规律，再回来继
>
> 续看，不管有没有找到规律，你对下面的话肯定更感触更深。
>
> ​	相信你已经观察了图片的规律了，下面我们说说我们发现的这个规律和
>
> 你所发现的是不是一样的。
>
> ​	百度文库的`PDF`文件，在每个页面下只显示3张图片的源代码，其他的图
>
> 片都没有，在页面向下移动的时候，之前的源代码是不断消失的，后面的
>
> 不断更新的。并且源代码存在这样的规律，每个页面下只有3张图片的
>
> `src`，并且在第一页面时的3张图为1，2，3，第二个页面的3张图为1，2，
>
> 3，第三页面的三张图为2，3，4，那么我们就可以以此类推嘛，找到保存
>
> 图片的规律，那就是先回到第二页把第1，2，3张图片全保存下来，然后再
>
> 换到第3页保存第4张图片，……
>
> ​	嘿嘿-我刚刚说的源代码，大家不会以为就是真正的图片代码的了吧？
>
> 哈哈，当然不是的-如果你按照上述的步骤进行了，你可以发现，我们其实
>
> 是获得了一大串东西，而真正的`URL`是在`url=(……)`,括号里面才是真正
>
> `URL`，那么我们怎么处理的？对，用正则表达式去获取就OK了。一样的，
>
> 想看具体怎么实现的，请直接看代码。
>
> ​	既然已经知道了我们获取页面总数的重要性，那么接下来就是获取页面
>
> 总数了。
>
> ​	在最开始的时候，没有考虑太多，因为很简单的就认为`Baidu`不会在这上
>
> 面设置反爬技术，就直接一个`xpath`获取了，然后在后面很多次测试中都发
>
> 现不行，捕捉不到目标，而且是对于同一个网页，有的时候行，有时候不
>
> 行！！？？？为啥??我最开始认为是电脑的加载问题哈--所以最先采取的是
>
> 利用`time`函数，停一会儿，额--不出大家所料，失败了。那这个时候我们
>
> 就认为可能是这个位置设置了反爬，不同的时候这个位置不同，为了解决
>
> 这个问题，我们测试了部分网页，然后把位置保存下来，发现零零碎碎的
>
> 有了四五种了，而且不是很多很多的测试，这不行啊，要是所有的都找出
>
> 来，不是有几百个?几个的话用`try`就可以，太多了，难道还用`try`吗？当
>
> 然是不可以的--
>
> ​	那么我们最终是怎么解决的？对，就是利用正则表达式直接对源代码进
>
> 行抓取，弄到我们的总页数……看到这里你可能会说，早这样不好吗?嗯-我
>
> 只能说这是我的一个小习惯吧-如果你没有这个习惯，每次都是用正则去捕
>
> 捉，那样更好，这个坑就完整地跳过了。
>
> ​	好了，现在我们的爬取工作差不多已经完成了，但是我们爬取的是网址
>
> 啊，并不是图片啊，也没有把图片用二进制流保存下来啊，怎么办呢？
>
> ​	如果你是第一次爬取图片的话，这个位置可能是一个坑，如果你已经有
>
> 经验了，那么这个位置就不是一个坑了。
>
> ​	我们这个位置可以用`request`向这些图片的地址进行访问，然后把返回结
>
> 构用二进制流保存下来就可以了。
>
> ​	但是，在保存这个位置，也有坑的，一定要关注的地方，就是文件的命
>
> 名，一定要用阿拉伯数字命名，如`1.jpg`，这样子，因为后面因为要转为
>
> `PDF`，不这样的话我们最终的PDF顺序会出错！
>
> ​	在最后的转`PDF`阶段呢，主要是借鉴了网上一位网友的转`PDF`程序，
>
> 嗯……我在运行过程中，对文件用标准`"RGB"`格式去修饰图片的时候一直报
>
> 错，然后用`"P"`格式去修饰图片的时候又可以了，这个原因是说我没
>
> 有"`RGB"`的模板？？为什么？？我也不知道--这可能是版本的问题吧，我用
>
> 的库的版本请见前面的`Before use`。
>
> ​	这里补充一点，一定要对文件进行排序，并且排序的过程我们这里采用
>
> 的方法很特别，详细请看源代码，这里解释一下为什么要排序。
>
> ​	原因其实很简单，因为你用程序去读取文件的时候并不是一个一个读
>
> 的，是并发的读取文件，也就是说小的文件容易先读进来，那么就会打乱
>
> 原来的图片的顺序。

### 小结

> 从一个做完的角度去看之前的过程，会觉得，当时真蠢啊，这些问题都会犯，什么什么的，这很正常，人都会这样想。
>
> 但是我觉得吧，我们没有权利站在现在的角度去看待过去的自己做了什么做了什么，这样是不公平的:smile:。
>
> 以前总觉得自己的爬虫挺厉害（注：此话为xkw言论，zll第一次接触爬虫），真正地做一个比较难的项目的时候，就感觉黔驴技穷了，人呐，永远不能自满。
>
> 对于自己想做的东西，一定要及时去做，而不是空想或者只看别人做，自己不做，因为只有做了，才能知道整个过程，才能学到东西。
>
> 还有如果想要做好一件事情，选好伙伴是非常重要的！在本项目的过程中无论少了我们两个中的任何一个人，该项目都不会如此顺利的完成！
>
> 互相鼓励，互相信任，是通往成功路上的利器:crossed_swords:



## 联系方式

> 本项目由向柯玮和周航于2020/4/27共同完成，如有疑惑请咨询
>
> xkw:xiasen99@gmail.com  
>
> zll:zh20010728@126.com   https://github.com/zll-hust



> HUST   xkw  &  zll   
